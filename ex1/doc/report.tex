%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode
\documentclass[11pt,a4paper]{article}

%\usepackage[left=70pt,top=50pt,bottom=70pt,right=40pt]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fixltx2e}
\usepackage{cmap}
\usepackage{enumerate}
\usepackage{ifthen}
\usepackage{listings}
\usepackage{url}
\usepackage[T1]{fontenc}
%\usepackage{fontspec}
%\usepackage{xunicode}
%\usepackage{xltxtra}
%\setmainfont[Mapping=tex-text,Ligatures={Common,Rare,Discretionary}]{Linux Libertine O}
\usepackage{pdflscape}
\usepackage{alltt}
\usepackage{algpseudocode}

\ifthenelse{\isundefined{\hypersetup}}{
    \usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue]{hyperref}
    \urlstyle{same}
}{}

\hypersetup{
    pdftitle={Intelligent Agents - EX1 - Yoan Blanc, Tiziano Signo}
}
\title{\phantomsection%
    A Reactive Agent for the Pickup and Delivery Problem
}
\author{
    Yoan Blanc \texttt{<yoan.blanc@epfl.ch>}, 213552\\
    Tiziano Signo \texttt{<tiziano.signo@epfl.ch>}, 226511
}
\date{\today}


\begin{document}
\maketitle

\noindent
\begin{quote}{\it

    At the beginning there are two tables:

    \begin{itemize}
        \item $p(i,j)$: The probability that in $\text{city}_i$ there is a task to be transported to $\text{city}_j$.
        \item $r(i,j)$: The reward for a task that is transported from $\text{city}_i$ to $\text{city}_j$.
    \end{itemize}

    \begin{enumerate}
        \item Define your state representation $S$, the possible actions $A$, the
            reward table $R(s,a) | s \in S, a \in A$ and the probability transition
            table $T(s,a,s') | s, s' in S, a \in A$.

        \item Implement the offline reinforcement learning algorithm for
            determining the actions to take in order to search and deliver
            tasks optimally. This algorithm should be executed before the
            vehicles start moving.

        \item Run simulations of one, two and three agents using your optimally
            learned strategy $V(S)$. Look at the performance graph of the agents.
            How does it change for different discount factor $\gamma$?

    \end{enumerate}
}\end{quote}

\newpage
\medskip
\textbf{State representation}

The space of states $S$ is defined by tuples of the current city ($curr$)
and the destination city ($dest$). We made this call because that what
is given to the agent as informations.

$$ \text{cities} = \{ \text{city}_1, \text{city}_2, \text{city}_3, \ldots, \text{city}_n \}$$

$$ S = \{ (curr, dest) \quad | \quad curr \in \text{cities}, dest \in
\text{cities} \cup \{ \emptyset \}, curr \neq dest \} $$

For example: $(city_i, city_j)$ represents the state where the agent is
in $city_i$ and there is a delivery to $city_j$ available to it. Likewise,
$(city_k, \emptyset)$ means that there are not tasks available to the agent
while it travels through $city_k$.

\medskip
\textbf{Action representation}

The space of actions $A$ is defined by accepting and delivering the proposed
task ($deliver$) or by deciding to move to another city.

$$ A = \{ action \quad | \quad action \in \text{cities} \cup \{\text{deliver}\} \} $$

\medskip
\textbf{Other definitions}

With the two spaces, we can now define the reward function $R(s,a)$ as well as
the state transition function $T(s,a,s')$ using $p(i,j)$ and $r(i,j)$. First,
define another function $q(i,j)$ which represents the cost to travel from
the $city_i$ to the $city_j$.

$$ q(i, j) = \text{cost per kilometer} * distance(city_i, city_j) $$

The reward function is defined as follow:

$$ R(s, a) = \left\{
    \begin{array}{l l}
        r(s.curr, s.dest) - q(s.curr, s.dest) & \text{if} \quad a \in \text{cities} \\
        q(s.curr, s.dest) & \text{if} \quad a = \text{deliver}
    \end{array} \right. $$

And the state transistion function:

% shall we only consider the neighbors city when not delivering packages?

$$ T(s, a, s') = \left\{
    \begin{array}{l l}
        p(s'.curr, s'.dest) & \text{if} \quad a = s'.curr \\
        p(s'.curr, s'.dest) & \text{if} \quad a = \text{deliver}, s.dest = s'.curr \\
        0 & \text{otherwise}
    \end{array} \right. $$

\medskip
\textbf{Algorithm}

We followed the algorithm from the slides. An extra variable $ Best$ is used to
store which $a$ was the $max$ one and not only the best value.

\begin{algorithmic}
    \Function{reinforcementLearning}{}
        \For{$s \in S$}
            \State $V(s) \gets 0$
        \EndFor
        \While{$error \leq \epsilon$}
            \For{$s \in S$}
                \For{$a \in A$}
                    $Q(s, a) \gets R(s, a) + \gamma \sum_{s' \in S} T(s,a,s') V(s')$
                \EndFor
            $V(s) \gets max_aQ(s, a)$
            \EndFor
        \EndWhile
    \EndFunction
\end{algorithmic}

\bigskip
\textbf{Implementation details}



\end{document}
